{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이게 가장 효과과 좋다.\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9836\n"
     ]
    }
   ],
   "source": [
    "# 문장 개수 체크 -> 이거는 문제가 모든 .을 찾을거니까\n",
    "file = open('data.txt','r')\n",
    "content =file.read()\n",
    "sentence = okt.morphs(content)\n",
    "count_sentence = len(re.findall(r'[.!?]', content))\n",
    "print(count_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장 완료\n"
     ]
    }
   ],
   "source": [
    "# 이거는 모든 형채소를 분석한 결과\n",
    "\n",
    "# 파일 읽기\n",
    "file = open('data.txt','r')\n",
    "content =file.read()\n",
    "\n",
    "# 문장 단위로 나누기\n",
    "sentences = re.split(r'[.!?]', content)\n",
    "\n",
    "# 각 문장을 형태소 분석하여 리스트에 저장\n",
    "morphed_sentences = [okt.morphs(sentence.strip()) for sentence in sentences if sentence.strip()]\n",
    "\n",
    "# 결과 파일 저장\n",
    "with open('final.txt', 'w') as output_file:\n",
    "    for i, morphs in enumerate(morphed_sentences, 1):\n",
    "        output_file.write(f\"{str(morphs)}\\n\")\n",
    "\n",
    "print(\"저장 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장완료\n",
      "총 문장 개수: 9330\n"
     ]
    }
   ],
   "source": [
    "# 이거는  명사만 가져오기\n",
    "\n",
    "# 파일 읽기\n",
    "with open('data.txt', 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Okt 객체 생성\n",
    "okt = Okt()\n",
    "\n",
    "# 정규 표현식을 사용하여 문장 단위로 분리 ('.', '?', '!' 기준)\n",
    "sentences = re.split(r'[.!?]', content)\n",
    "\n",
    "# 각 문장을 형태소 분석하여 리스트에 저장\n",
    "morphed_sentences = [okt.nouns(sentence.strip()) for sentence in sentences if sentence.strip()]\n",
    "\n",
    "with open('final.txt', 'w') as output_file:\n",
    "    for i, nouns in enumerate(morphed_sentences, 1):\n",
    "        output_file.write(f\"문장 {i}: {nouns}\\n\")\n",
    "    print(\"저장완료\")\n",
    "\n",
    "\n",
    "# 문장 개수 출력\n",
    "print(f\"총 문장 개수: {len(morphed_sentences)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문을 쓰는거\n",
    "\n",
    "\n",
    "with open('final_testing.txt', 'w') as output_file:\n",
    "    for i, morphs in enumerate(sentences, 1):\n",
    "        output_file.write(f\"문장 {i}: {morphs}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_files\\final_original_part_1.txt 및 split_files\\final_nouns_part_1.txt 저장 완료 (1 ~ 500)\n",
      "split_files\\final_original_part_2.txt 및 split_files\\final_nouns_part_2.txt 저장 완료 (501 ~ 1000)\n",
      "split_files\\final_original_part_3.txt 및 split_files\\final_nouns_part_3.txt 저장 완료 (1001 ~ 1500)\n",
      "split_files\\final_original_part_4.txt 및 split_files\\final_nouns_part_4.txt 저장 완료 (1501 ~ 2000)\n",
      "split_files\\final_original_part_5.txt 및 split_files\\final_nouns_part_5.txt 저장 완료 (2001 ~ 2500)\n",
      "split_files\\final_original_part_6.txt 및 split_files\\final_nouns_part_6.txt 저장 완료 (2501 ~ 3000)\n",
      "split_files\\final_original_part_7.txt 및 split_files\\final_nouns_part_7.txt 저장 완료 (3001 ~ 3500)\n",
      "split_files\\final_original_part_8.txt 및 split_files\\final_nouns_part_8.txt 저장 완료 (3501 ~ 4000)\n",
      "split_files\\final_original_part_9.txt 및 split_files\\final_nouns_part_9.txt 저장 완료 (4001 ~ 4500)\n",
      "split_files\\final_original_part_10.txt 및 split_files\\final_nouns_part_10.txt 저장 완료 (4501 ~ 5000)\n",
      "split_files\\final_original_part_11.txt 및 split_files\\final_nouns_part_11.txt 저장 완료 (5001 ~ 5500)\n",
      "split_files\\final_original_part_12.txt 및 split_files\\final_nouns_part_12.txt 저장 완료 (5501 ~ 6000)\n",
      "split_files\\final_original_part_13.txt 및 split_files\\final_nouns_part_13.txt 저장 완료 (6001 ~ 6500)\n",
      "split_files\\final_original_part_14.txt 및 split_files\\final_nouns_part_14.txt 저장 완료 (6501 ~ 7000)\n",
      "split_files\\final_original_part_15.txt 및 split_files\\final_nouns_part_15.txt 저장 완료 (7001 ~ 7500)\n",
      "split_files\\final_original_part_16.txt 및 split_files\\final_nouns_part_16.txt 저장 완료 (7501 ~ 8000)\n",
      "split_files\\final_original_part_17.txt 및 split_files\\final_nouns_part_17.txt 저장 완료 (8001 ~ 8500)\n",
      "split_files\\final_original_part_18.txt 및 split_files\\final_nouns_part_18.txt 저장 완료 (8501 ~ 9000)\n",
      "split_files\\final_original_part_19.txt 및 split_files\\final_nouns_part_19.txt 저장 완료 (9001 ~ 9330)\n",
      "총 문장 개수: 9330\n"
     ]
    }
   ],
   "source": [
    "# 이거는 쪼개서 저장할떄\n",
    "\n",
    "\n",
    "# 파일 읽기\n",
    "with open('data.txt', 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Okt 객체 생성\n",
    "okt = Okt()\n",
    "\n",
    "# 정규 표현식을 사용하여 문장 단위로 분리 ('.', '?', '!' 기준)\n",
    "sentences = re.split(r'[.!?]', content)\n",
    "\n",
    "# 원문과 명사 추출 결과 저장\n",
    "original_sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "nouns_sentences = [okt.nouns(sentence) for sentence in original_sentences]  # 명사만 추출\n",
    "\n",
    "# 총 문장 개수 및 나눌 크기 설정\n",
    "total_sentences = len(original_sentences)  # 문장 개수\n",
    "chunk_size = 500  # 500개씩 나누기\n",
    "\n",
    "# 저장할 디렉토리 생성 (없으면 생성)\n",
    "output_dir = \"split_files\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 500개씩 나누어 저장\n",
    "for idx, start in enumerate(range(0, total_sentences, chunk_size)):\n",
    "    end = start + chunk_size\n",
    "\n",
    "    # 원문 저장\n",
    "    original_filename = os.path.join(output_dir, f\"final_original_part_{idx+1}.txt\")\n",
    "    with open(original_filename, \"w\", encoding=\"utf-8\") as orig_file:\n",
    "        for i, sentence in enumerate(original_sentences[start:end], start + 1):\n",
    "            orig_file.write(f\"문장 {i}: {sentence}\\n\")\n",
    "\n",
    "    # 명사 추출 결과 저장\n",
    "    nouns_filename = os.path.join(output_dir, f\"final_nouns_part_{idx+1}.txt\")\n",
    "    with open(nouns_filename, \"w\", encoding=\"utf-8\") as nouns_file:\n",
    "        for i, nouns in enumerate(nouns_sentences[start:end], start + 1):\n",
    "            nouns_file.write(f\"문장 {i}: {nouns}\\n\")\n",
    "\n",
    "    print(f\"{original_filename} 및 {nouns_filename} 저장 완료 ({start+1} ~ {min(end, total_sentences)})\")\n",
    "\n",
    "# 총 문장 개수 출력\n",
    "print(f\"총 문장 개수: {total_sentences}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 단어 빈도수 분석 완료! CSV 파일 저장: everything.csv\n"
     ]
    }
   ],
   "source": [
    "# 모든형태소를 숫자로 변환\n",
    "\n",
    "file = open('data.txt','r')\n",
    "content =file.read()\n",
    "\n",
    "# Okt 객체 생성\n",
    "okt = Okt()\n",
    "\n",
    "# 문장 단위로 나누기\n",
    "sentences = re.split(r'[.!?]', content)\n",
    "\n",
    "word_counts = Counter()\n",
    "\n",
    "# 각 문장에서 형태소 분석 후 빈도수 계산\n",
    "for sentence in sentences:\n",
    "    if sentence.strip():  # 빈 문자열 제외\n",
    "        words = okt.morphs(sentence.strip())  # 형태소 분석\n",
    "        word_counts.update(words)  # 단어 빈도수 업데이트\n",
    "\n",
    "# 단어 빈도수를 DataFrame으로 변환\n",
    "df = pd.DataFrame(word_counts.items(), columns=[\"단어\", \"빈도수\"])\n",
    "\n",
    "# 빈도수 기준으로 정렬 (내림차순)\n",
    "df = df.sort_values(by=\"빈도수\", ascending=False)\n",
    "\n",
    "# CSV 파일로 저장\n",
    "output_csv = \"everything.csv\"\n",
    "df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"✅ 단어 빈도수 분석 완료! CSV 파일 저장: {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 단어 빈도수 분석 완료! CSV 파일 저장: nouns.csv\n"
     ]
    }
   ],
   "source": [
    "# 명사만 변환\n",
    "\n",
    "\n",
    "file = open('data.txt','r')\n",
    "content =file.read()\n",
    "\n",
    "# Okt 객체 생성\n",
    "okt = Okt()\n",
    "\n",
    "# 문장 단위로 나누기\n",
    "sentences = re.split(r'[.!?]', content)\n",
    "\n",
    "word_counts = Counter()\n",
    "\n",
    "# 각 문장에서 형태소 분석 후 빈도수 계산\n",
    "for sentence in sentences:\n",
    "    if sentence.strip():  # 빈 문자열 제외\n",
    "        words = okt.nouns(sentence.strip())  # 형태소 분석\n",
    "        word_counts.update(words)  # 단어 빈도수 업데이트\n",
    "\n",
    "# 단어 빈도수를 DataFrame으로 변환\n",
    "df = pd.DataFrame(word_counts.items(), columns=[\"단어\", \"빈도수\"])\n",
    "\n",
    "# 빈도수 기준으로 정렬 (내림차순)\n",
    "df = df.sort_values(by=\"빈도수\", ascending=False)\n",
    "\n",
    "# CSV 파일로 저장\n",
    "output_csv = \"nouns.csv\"\n",
    "df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"✅ 단어 빈도수 분석 완료! CSV 파일 저장: {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이제 이거를 분석을 해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 해야 하는거는 문장 단위로 쪼개는건데"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

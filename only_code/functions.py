import os
import re
from pykospacing import Spacing
from sentence_transformers import SentenceTransformer, util
import pandas as pd
import numpy as np
# import ace_tools as tools
import ace_tools_open as tools
import pickle
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations
from collections import defaultdict
from konlpy.tag import Okt
#------------------------------------------------------------------------------
def rawdata_to_cleandata(data_file):
    with open(f'{data_file}', 'r', encoding='utf-8') as file:
        content = file.read()

    content = "".join(content.split())
    sentences = re.split(r'[.!?]', content)

    # ë¹ˆ ë¬¸ì¥ ì œê±° ë° í°ë”°ì˜´í‘œ ì œê±°
    clean_sentences = [re.sub(r'"', '', sentence) for sentence in sentences if sentence]

    spacing = Spacing()
    
    sentences = clean_sentences
    # ê³µë°± ë³µì›
    spaced_sentences = [spacing(sentence.strip()) for sentence in sentences]
    with open('clean_data.txt', 'w', encoding='utf-8') as output_file:
        output_file.write("\n".join(spaced_sentences))
#------------------------------------------------------------------------------
def book_to_pkl_nouns(data_file):
    file_path = f"{data_file}"
    with open(file_path, "r", encoding="utf-8") as f:
        lines = f.readlines()  # ì¤„ ë‹¨ìœ„ë¡œ ì½ê¸°

    stopwords = {}

    okt = Okt()
    tokenized_sentences = []
    filtered_words = []  # ë¶ˆìš©ì–´ ì œê±°ëœ ë‹¨ì–´ë§Œ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸

    for idx, line in enumerate(lines):
        nouns = okt.nouns(line)  # ëª…ì‚¬ë§Œ ì¶”ì¶œ

        # ğŸ”¹ ë¶ˆìš©ì–´ ì œê±° (ì»´í”„ë¦¬í—¨ì…˜ ì—†ì´ ì‘ì„±)
        filtered_nouns = []
        for word in nouns:
            if word not in stopwords:
                filtered_nouns.append(word)

        # filtered_nouns = list(set(filtered_nouns))  # ì¤‘ë³µ ì œê±°-> í•œ ë¬¸ì¥ì— ì—¬ëŸ¬ê°œì˜ ê²ƒì´ ìˆë”ë¼ë„ ë¬´ì‹œ
             #ë‚˜ëŠ” ì´ê²Œ êµ³ì´ ìˆì–´ì•¼ í•˜ë‚˜ ì‹¶ê¸°ëŠ” í•¨

        tokenized_sentences.append({"index": idx, "num_nouns": len(filtered_nouns), "nouns": filtered_nouns})
        
        # ëª¨ë“  ë‹¨ì–´ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        filtered_words.extend(filtered_nouns)

    # ê²°ê³¼ ì €ì¥ (ë¶ˆìš©ì–´ ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥)
    with open("nouns_lines.pkl", "wb") as f:
        pickle.dump(tokenized_sentences, f)
#------------------------------------------------------------------------------
#------------------------------------------------------------------------------
def nouns_ì§€ì§€ë„(pkl_file):
    with open("nouns_lines.pkl", "rb") as f:
        sentences = pickle.load(f)

    word_pairs = defaultdict(int)# ì²˜ìŒ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ ìŒ(pair)ë„ ì•ˆì „í•˜ê²Œ ì¹´ìš´íŠ¸í•  ìˆ˜ ìˆìŒ
    # ì´ë”°ê°€  word_pairs[pair] += 1 ì´ê±° ë–„ë¬¸
    # ì¼ë‹¨ ì—¬ê¸°ì— ìŒìœ¼ë¡œ ì €ì¥ì„ í•¨

    total_sentences = len(sentences)


    for entry in sentences:
        nouns = entry["nouns"]
        for pair in combinations(nouns, 2):  # ë‹¨ì–´ ìŒ ìƒì„±
            word_pairs[pair] += 1

    # ì§€ì§€ë„ ê³„ì‚°
    support_values = {}  # ë¹ˆ ë”•ì…”ë„ˆë¦¬ ìƒì„±

    for pair, count in word_pairs.items():
        support_values[pair] = count / total_sentences  # ì§€ì§€ë„ ê³„ì‚° í›„ ë”•ì…”ë„ˆë¦¬ì— ì¶”ê°€

    # ì—¬ê¸°ì„œ pairì€ ë‹¨ì–´ ìŒì´ê³ 
    # ì—¬ê¸°ì„œ countëŠ” ë“±ì¥ íšŸìˆ˜ì´ë‹¤.
    # ì¦‰ íŠ¹ì • ë‹¨ì–´ ìŒ == ë“±ì¥íšŸìˆ˜/ì „ì²´ë¬¸ì¥ìˆ˜

    df = pd.DataFrame(support_values.items(), columns=["ë‹¨ì–´ìŒ", "ì§€ì§€ë„"])
    df = df.sort_values(by="ì§€ì§€ë„", ascending=False)

    df.to_csv("ëª…ì‚¬_ì§€ì§€ë„.csv", index=False, encoding="utf-8-sig")
#------------------------------------------------------------------------------
def nouns_ì‹ ë¢°ë„(pkl_file):
    with open(pkl_file, "rb") as f:
        sentences = pickle.load(f)

   # ëª…ì‚¬ ë“±ì¥ íšŸìˆ˜ ë° ì¡°í•© ë¹ˆë„ ê³„ì‚°
    word_pairs = defaultdict(int)
    word_counts = defaultdict(int)
    # ì´ê±°ëŠ” ì²˜ìŒ ë“±ì¥í•˜ëŠ” ìŒ(pairs)ë„ ì•ˆì „í•˜ê²Œ ì¹´ìš´íŠ¸ë¥¼ í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” ì½”ë“œ
    # -> ê¸°ì¡´ì˜ dictë¥¼ ì“°ë©´ ì—†ëŠ”ê±°ì— ëŒ€í•´ 0ìœ¼ë¡œ ì²˜ë¦¬ë¥¼ ì•ˆí•´ì£¼ê¸° ë•Œë¬¸ì— ë¬¸ì œê°€ ìƒê¸°ëŠ” ê²ƒì´ë‹¤. 
    total_sentences = len(sentences)

    for entry in sentences:
        nouns = entry["nouns"]  # ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°

        # ê°œë³„ ëª…ì‚¬ ë“±ì¥ íšŸìˆ˜ ê³„ì‚° -> ì´ê±°ëŠ” í•˜ë‚˜ ì¦‰ A,B ì´ëŸ°ê±°êµ¬ë‚˜
        for word in nouns:
            word_counts[word] += 1


        # ëª…ì‚¬ ìŒ ë“±ì¥ íšŸìˆ˜ ê³„ì‚° -> ê·¸ëŸ¬ë©´ ì´ê²Œ ì „ì²´êµ¬ë‚˜ ì¦‰ A,Bë‘˜ë‹¤ í•¨ê»˜ ë“±ì¥í•œ ìˆ˜
        for pair in combinations(nouns, 2):
            word_pairs[pair] += 1

    # ì‹ ë¢°ë„ ê³„ì‚°
    confidence_values = []
    for pair, support_AB in word_pairs.items():
        A, B = pair
        # ì´ê±°ëŠ” ì™œ í•œê±°ì•¼?

        support_A = word_counts.get(A, 0)   # Aì˜ ì§€ì§€ë„(ë“±ì¥í•œ íšŸìˆ˜)-> í‚¤ ê°’ìœ¼ë¡œ ê°’ì„ ê°€ì ¸ì˜¤ëŠ” ë°©ì‹
        support_B = word_counts.get(B, 0)   # Bì˜ ì§€ì§€ë„(ë“±ì¥í•œ íšŸìˆ˜)


        confidence_A_to_B = support_AB / support_A if support_A > 0 else 0# ì´ê±°ëŠ” í˜¹ì‹œ 0ìœ¼ë¡œ ë‚˜ëˆŒê¹Œë´ ë§Œë“  ì˜ˆë¹„ ì¥ì¹˜
        confidence_B_to_A = support_AB / support_B if support_B > 0 else 0# ì´ê±°ëŠ” í˜¹ì‹œ 0ìœ¼ë¡œ ë‚˜ëˆŒê¹Œë´ ë§Œë“  ì˜ˆë¹„ ì¥ì¹˜

        confidence_values.append((pair, confidence_A_to_B, confidence_B_to_A))

    confidence_df = pd.DataFrame(confidence_values, columns=["ë‹¨ì–´ìŒ", "ì‹ ë¢°ë„(Aâ†’B)", "ì‹ ë¢°ë„(Bâ†’A)"])

    # CSV ì €ì¥
    confidence_df.to_csv("ëª…ì‚¬_ì‹ ë¢°ë„.csv", index=False, encoding="utf-8-sig")
#------------------------------------------------------------------------------
def nouns_í–¥ìƒë„(pkl_file):
    with open(pkl_file, "rb") as f:
        sentences = pickle.load(f)

    # ëª…ì‚¬ ë“±ì¥ íšŸìˆ˜ ë° ì¡°í•© ë¹ˆë„ ê³„ì‚°
    word_counts = defaultdict(int)
    total_sentences = len(sentences)

    for entry in sentences:
        nouns = entry["nouns"]
        for word in nouns:
            word_counts[word] += 1

    # ì‹ ë¢°ë„ ë¶ˆëŸ¬ì˜¤ê¸°
    confidence_df = pd.read_csv("ëª…ì‚¬_ì‹ ë¢°ë„.csv")
    lift_values = []

    for index, row in confidence_df.iterrows():
        pair = eval(row["ë‹¨ì–´ìŒ"])  # ë¬¸ìì—´ì„ íŠœí”Œë¡œ ë³€í™˜
        confidence_A_to_B = row["ì‹ ë¢°ë„(Aâ†’B)"]# ì´ ì‹ ë¢°ë„ëŠ” ì™œ ê°€ì ¸ì™”ì„ê¹Œ?
        confidence_B_to_A = row["ì‹ ë¢°ë„(Bâ†’A)"]# ì´ ì‹ ë¢°ë„ëŠ” ì™œ ê°€ì ¸ì™”ì„ê¹Œ?

        A, B = pair

        support_B = word_counts.get(B, 0) / total_sentences  # Bì˜ ì§€ì§€ë„
        support_A = word_counts.get(A, 0) / total_sentences  # Aì˜ ì§€ì§€ë„
    # ì´ê±°ë¥¼ ì—­ìœ¼ë¡œ ë´ì•¼ì§€ ì´í•´ê°€ ë˜ëŠ”êµ¬ë‚˜

        lift_A_to_B = confidence_A_to_B / support_B if support_B > 0 else 0
        lift_B_to_A = confidence_B_to_A / support_A if support_A > 0 else 0

        lift_values.append((pair, lift_A_to_B, lift_B_to_A))

    lift_df = pd.DataFrame(lift_values, columns=["ë‹¨ì–´ìŒ", "í–¥ìƒë„(Aâ†’B)", "í–¥ìƒë„(Bâ†’A)"])

    # CSV ì €ì¥
    lift_df.to_csv("ëª…ì‚¬_í–¥ìƒë„.csv", index=False, encoding="utf-8-sig")
#------------------------------------------------------------------------------
def combin_three_nouns():
    # ğŸ”¹ CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
    support_df = pd.read_csv("ëª…ì‚¬_ì§€ì§€ë„.csv")
    confidence_df = pd.read_csv("ëª…ì‚¬_ì‹ ë¢°ë„.csv")
    lift_df = pd.read_csv("ëª…ì‚¬_í–¥ìƒë„.csv")

    # ğŸ”¹ ê°€ì¤‘ì¹˜ ì„¤ì • (ì¶”ì²œ: Liftì™€ Confidenceë¥¼ ë” ê°•ì¡°)
    weight_support = 0.2
    weight_confidence = 0.4
    weight_lift = 0.4
    def predict_next_words(target_word, top_n=10):
        # ğŸ”¹ ë‹¨ì–´ ìŒì´ í¬í•¨ëœ ë°ì´í„° í•„í„°ë§
        filtered_confidence = confidence_df[confidence_df["ë‹¨ì–´ìŒ"].str.contains(target_word)]
        filtered_lift = lift_df[lift_df["ë‹¨ì–´ìŒ"].str.contains(target_word)]
        filtered_support = support_df[support_df["ë‹¨ì–´ìŒ"].str.contains(target_word)]

        scores = {}

        # ğŸ”¹ í•„í„°ë§ëœ ë°ì´í„°ì—ì„œ ì—°ê´€ ë‹¨ì–´ ì°¾ê¸°
        for _, row in filtered_confidence.iterrows():  
            pair = eval(row["ë‹¨ì–´ìŒ"])  # ë¬¸ìì—´ì„ íŠœí”Œë¡œ ë³€í™˜
            confidence_A_to_B = row["ì‹ ë¢°ë„(Aâ†’B)"]
            confidence_B_to_A = row["ì‹ ë¢°ë„(Bâ†’A)"]

            # ğŸ”¹ ëŒ€ìƒ ë‹¨ì–´ì™€ ì§ì´ ëœ ë‹¨ì–´ ì°¾ê¸°
            other_word = pair[1] if pair[0] == target_word else pair[0]

            # ğŸ”¹ íŠ¹ì • ë‹¨ì–´ ìŒì— í•´ë‹¹í•˜ëŠ” í–¥ìƒë„ ê°’ ì°¾ê¸°
            lift_row = filtered_lift[filtered_lift["ë‹¨ì–´ìŒ"] == str(pair)]
            if not lift_row.empty:
                lift_A_to_B = lift_row["í–¥ìƒë„(Aâ†’B)"].values[0]
                lift_B_to_A = lift_row["í–¥ìƒë„(Bâ†’A)"].values[0]
            else:
                lift_A_to_B = lift_B_to_A = 0  # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ 0

            # ğŸ”¹ íŠ¹ì • ë‹¨ì–´ ìŒì— í•´ë‹¹í•˜ëŠ” ì§€ì§€ë„ ê°’ ì°¾ê¸°
            support_row = filtered_support[filtered_support["ë‹¨ì–´ìŒ"] == str(pair)]
            support_AB = support_row["ì§€ì§€ë„"].values[0] if not support_row.empty else 0

            # ğŸ”¹ ìµœì¢… ì ìˆ˜ ê³„ì‚°
            score = (weight_support * support_AB) + \
                    (weight_confidence * max(confidence_A_to_B, confidence_B_to_A)) + \
                    (weight_lift * max(lift_A_to_B, lift_B_to_A))

            scores[other_word] = score

        # ğŸ”¹ ìƒìœ„ Nê°œ ë‹¨ì–´ ì •ë ¬í•˜ì—¬ ë°˜í™˜
        sorted_words = sorted(scores.items(), key=lambda item: item[1], reverse=True)[:top_n]
        
        return sorted_words
    # ğŸ”¹ ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
    def combin_three():
        target_word = input("ì˜ˆì¸¡í•  ë‹¨ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
        top_predictions = predict_next_words(target_word)

        # ğŸ”¹ ê²°ê³¼ ì¶œë ¥
        print(f"\n'{target_word}' ë‹¤ìŒì— ë‚˜ì˜¬ ê°€ëŠ¥ì„±ì´ ë†’ì€ ë‹¨ì–´ Top 10:")
        for word, score in top_predictions:
            print(f"{word}: {score:.4f}")

    # ì‹¤í–‰
    combin_three()
#------------------------------------------------------------------------------
#------------------------------------------------------------------------------
#------------------------------------------------------------------------------
#------------------------------------------------------------------------------
#------------------------------------------------------------------------------
def book_to_pkl_morphs(data_file):
    file_path = f"{data_file}"
    with open(file_path, "r", encoding="utf-8") as f:
        lines = f.readlines()  # ì¤„ ë‹¨ìœ„ë¡œ ì½ê¸°

    stopwords = {}

    okt = Okt()
    tokenized_sentences = []
    filtered_words = []  # ë¶ˆìš©ì–´ ì œê±°ëœ ë‹¨ì–´ë§Œ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸

    for idx, line in enumerate(lines):
        morphs = okt.morphs(line)  # ëª¨ë“  í˜•íƒœì†Œ ì¶”ì¶œ

        # ğŸ”¹ ë¶ˆìš©ì–´ ì œê±° (ì»´í”„ë¦¬í—¨ì…˜ ì—†ì´ ì‘ì„±)
        filtered_morphs = []
        for word in morphs:
            if word not in stopwords:
                filtered_morphs.append(word)


        tokenized_sentences.append({
            "index": idx,
            "num_morphs": len(filtered_morphs),
            "morphs": filtered_morphs
        })
        
        # ëª¨ë“  ë‹¨ì–´ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        filtered_words.extend(filtered_morphs)

    # ê²°ê³¼ ì €ì¥ (ë¶ˆìš©ì–´ ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥)
    with open("morphs_lines.pkl", "wb") as f:
        pickle.dump(tokenized_sentences, f)
#------------------------------------------------------------------------------
def morphs_ì§€ì§€ë„(pkl_file):
    with open(pkl_file, "rb") as f:
        sentences = pickle.load(f)  # í˜•íƒœì†Œ ë¶„ì„ëœ ë¬¸ì¥ ë°ì´í„° ë¡œë“œ

    word_pairs = defaultdict(int)  # ë‹¨ì–´ ìŒ ì¹´ìš´íŠ¸ ë”•ì…”ë„ˆë¦¬
    total_sentences = len(sentences)  # ì „ì²´ ë¬¸ì¥ ìˆ˜

    for entry in sentences:
        morphs = entry["morphs"]  # í˜•íƒœì†Œ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        for pair in combinations(morphs, 2):  # í˜•íƒœì†Œ ìŒ ìƒì„±
            word_pairs[pair] += 1

    # ğŸ”¹ ì§€ì§€ë„ ê³„ì‚°
    support_values = {pair: count / total_sentences for pair, count in word_pairs.items()}

    # ğŸ”¹ ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ì €ì¥
    df = pd.DataFrame(support_values.items(), columns=["í˜•íƒœì†ŒìŒ", "ì§€ì§€ë„"])
    df = df.sort_values(by="ì§€ì§€ë„", ascending=False)

    df.to_csv("í˜•íƒœì†Œ_ì§€ì§€ë„.csv", index=False, encoding="utf-8-sig")
#------------------------------------------------------------------------------
def morphs_ì‹ ë¢°ë„(pkl_file):
    with open(pkl_file, "rb") as f:
        sentences = pickle.load(f)  # í˜•íƒœì†Œ ë¶„ì„ëœ ë¬¸ì¥ ë°ì´í„° ë¡œë“œ

    # ğŸ”¹ í˜•íƒœì†Œ ë“±ì¥ íšŸìˆ˜ ë° ì¡°í•© ë¹ˆë„ ê³„ì‚°
    word_pairs = defaultdict(int)  # í˜•íƒœì†Œ ìŒ ë“±ì¥ íšŸìˆ˜ ì €ì¥
    word_counts = defaultdict(int)  # ê°œë³„ í˜•íƒœì†Œ ë“±ì¥ íšŸìˆ˜ ì €ì¥
    total_sentences = len(sentences)  # ì „ì²´ ë¬¸ì¥ ìˆ˜

    for entry in sentences:
        morphs = entry["morphs"]  # í˜•íƒœì†Œ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°

        # ğŸ”¹ ê°œë³„ í˜•íƒœì†Œ ë“±ì¥ íšŸìˆ˜ ê³„ì‚°
        for word in morphs:
            word_counts[word] += 1

        # ğŸ”¹ í˜•íƒœì†Œ ìŒ ë“±ì¥ íšŸìˆ˜ ê³„ì‚°
        for pair in combinations(morphs, 2):
            word_pairs[pair] += 1

    # ğŸ”¹ ì‹ ë¢°ë„ ê³„ì‚°
    confidence_values = []
    for pair, support_AB in word_pairs.items():
        A, B = pair

        support_A = word_counts.get(A, 0)  # Aì˜ ë“±ì¥ íšŸìˆ˜
        support_B = word_counts.get(B, 0)  # Bì˜ ë“±ì¥ íšŸìˆ˜

        # A â†’ B ì‹ ë¢°ë„
        confidence_A_to_B = support_AB / support_A if support_A > 0 else 0
        # B â†’ A ì‹ ë¢°ë„
        confidence_B_to_A = support_AB / support_B if support_B > 0 else 0

        confidence_values.append((pair, confidence_A_to_B, confidence_B_to_A))

    # ğŸ”¹ ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ì €ì¥
    confidence_df = pd.DataFrame(confidence_values, columns=["í˜•íƒœì†ŒìŒ", "ì‹ ë¢°ë„(Aâ†’B)", "ì‹ ë¢°ë„(Bâ†’A)"])
    confidence_df.to_csv("í˜•íƒœì†Œ_ì‹ ë¢°ë„.csv", index=False, encoding="utf-8-sig")
#------------------------------------------------------------------------------
def morphs_í–¥ìƒë„(pkl_file):
    with open(pkl_file, "rb") as f:
        sentences = pickle.load(f)  # í˜•íƒœì†Œ ë¶„ì„ëœ ë¬¸ì¥ ë°ì´í„° ë¡œë“œ

    # ğŸ”¹ í˜•íƒœì†Œ ë“±ì¥ íšŸìˆ˜ ê³„ì‚°
    word_counts = defaultdict(int)  # ê°œë³„ í˜•íƒœì†Œ ë“±ì¥ íšŸìˆ˜ ì €ì¥
    total_sentences = len(sentences)  # ì „ì²´ ë¬¸ì¥ ìˆ˜

    for entry in sentences:
        morphs = entry["morphs"]  # í˜•íƒœì†Œ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        for word in morphs:
            word_counts[word] += 1  # ê°œë³„ í˜•íƒœì†Œ ë“±ì¥ íšŸìˆ˜ ì¦ê°€

    # ğŸ”¹ ì‹ ë¢°ë„ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° (í˜•íƒœì†Œ_ì‹ ë¢°ë„.csv)
    confidence_df = pd.read_csv("í˜•íƒœì†Œ_ì‹ ë¢°ë„.csv")
    lift_values = []

    for index, row in confidence_df.iterrows():
        pair = eval(row["í˜•íƒœì†ŒìŒ"])  # ë¬¸ìì—´ì„ íŠœí”Œë¡œ ë³€í™˜
        confidence_A_to_B = row["ì‹ ë¢°ë„(Aâ†’B)"]
        confidence_B_to_A = row["ì‹ ë¢°ë„(Bâ†’A)"]

        A, B = pair

        # ğŸ”¹ ê°œë³„ í˜•íƒœì†Œ(A, B)ì˜ ì§€ì§€ë„ ê³„ì‚°
        support_B = word_counts.get(B, 0) / total_sentences  # Bì˜ ì§€ì§€ë„
        support_A = word_counts.get(A, 0) / total_sentences  # Aì˜ ì§€ì§€ë„

        # ğŸ”¹ í–¥ìƒë„(A â†’ B, B â†’ A) ê³„ì‚°
        lift_A_to_B = confidence_A_to_B / support_B if support_B > 0 else 0
        lift_B_to_A = confidence_B_to_A / support_A if support_A > 0 else 0

        lift_values.append((pair, lift_A_to_B, lift_B_to_A))

    # ğŸ”¹ ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ì €ì¥
    lift_df = pd.DataFrame(lift_values, columns=["í˜•íƒœì†ŒìŒ", "í–¥ìƒë„(Aâ†’B)", "í–¥ìƒë„(Bâ†’A)"])
    lift_df.to_csv("í˜•íƒœì†Œ_í–¥ìƒë„.csv", index=False, encoding="utf-8-sig")
#------------------------------------------------------------------------------
def combin_three_morphs():
    # ğŸ”¹ CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° (í˜•íƒœì†Œ ê¸°ë°˜ ë°ì´í„°)
    support_df = pd.read_csv("í˜•íƒœì†Œ_ì§€ì§€ë„.csv")
    confidence_df = pd.read_csv("í˜•íƒœì†Œ_ì‹ ë¢°ë„.csv")
    lift_df = pd.read_csv("í˜•íƒœì†Œ_í–¥ìƒë„.csv")

    # ğŸ”¹ ê°€ì¤‘ì¹˜ ì„¤ì • (Liftì™€ Confidenceë¥¼ ë” ê°•ì¡°)
    weight_support = 0.2
    weight_confidence = 0.4
    weight_lift = 0.4

    def predict_next_words(target_word, top_n=10):
        # ğŸ”¹ íŠ¹ì • ë‹¨ì–´ê°€ í¬í•¨ëœ ë°ì´í„° í•„í„°ë§
        filtered_confidence = confidence_df[confidence_df["í˜•íƒœì†ŒìŒ"].str.contains(target_word)]
        filtered_lift = lift_df[lift_df["í˜•íƒœì†ŒìŒ"].str.contains(target_word)]
        filtered_support = support_df[support_df["í˜•íƒœì†ŒìŒ"].str.contains(target_word)]

        scores = {}

        # ğŸ”¹ í•„í„°ë§ëœ ë°ì´í„°ì—ì„œ ì—°ê´€ëœ í˜•íƒœì†Œ ì°¾ê¸°
        for _, row in filtered_confidence.iterrows():
            pair = eval(row["í˜•íƒœì†ŒìŒ"])  # ë¬¸ìì—´ì„ íŠœí”Œë¡œ ë³€í™˜
            confidence_A_to_B = row["ì‹ ë¢°ë„(Aâ†’B)"]
            confidence_B_to_A = row["ì‹ ë¢°ë„(Bâ†’A)"]

            # ğŸ”¹ ëŒ€ìƒ ë‹¨ì–´ì™€ ì§ì´ ëœ í˜•íƒœì†Œ ì°¾ê¸°
            other_word = pair[1] if pair[0] == target_word else pair[0]

            # ğŸ”¹ íŠ¹ì • í˜•íƒœì†Œ ìŒì— ëŒ€í•œ í–¥ìƒë„ ê°’ ì°¾ê¸°
            lift_row = filtered_lift[filtered_lift["í˜•íƒœì†ŒìŒ"] == str(pair)]
            if not lift_row.empty:
                lift_A_to_B = lift_row["í–¥ìƒë„(Aâ†’B)"].values[0]
                lift_B_to_A = lift_row["í–¥ìƒë„(Bâ†’A)"].values[0]
            else:
                lift_A_to_B = lift_B_to_A = 0  # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ 0

            # ğŸ”¹ íŠ¹ì • í˜•íƒœì†Œ ìŒì— ëŒ€í•œ ì§€ì§€ë„ ê°’ ì°¾ê¸°
            support_row = filtered_support[filtered_support["í˜•íƒœì†ŒìŒ"] == str(pair)]
            support_AB = support_row["ì§€ì§€ë„"].values[0] if not support_row.empty else 0

            # ğŸ”¹ ìµœì¢… ì ìˆ˜ ê³„ì‚°
            score = (weight_support * support_AB) + \
                    (weight_confidence * max(confidence_A_to_B, confidence_B_to_A)) + \
                    (weight_lift * max(lift_A_to_B, lift_B_to_A))

            scores[other_word] = score

        # ğŸ”¹ ìƒìœ„ Nê°œ ë‹¨ì–´ ì •ë ¬í•˜ì—¬ ë°˜í™˜
        sorted_words = sorted(scores.items(), key=lambda item: item[1], reverse=True)[:top_n]
        
        return sorted_words

    # ğŸ”¹ ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
    target_word = input("ì˜ˆì¸¡í•  í˜•íƒœì†Œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
    top_predictions = predict_next_words(target_word)

    # ğŸ”¹ ê²°ê³¼ ì¶œë ¥
    print(f"\n'{target_word}' ë‹¤ìŒì— ë‚˜ì˜¬ ê°€ëŠ¥ì„±ì´ ë†’ì€ í˜•íƒœì†Œ Top 10:")
    for word, score in top_predictions:
        print(f"{word}: {score:.4f}")
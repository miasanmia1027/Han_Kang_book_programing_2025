import os
import re
from pykospacing import Spacing
from sentence_transformers import SentenceTransformer, util
import pandas as pd
import numpy as np
# import ace_tools as tools
import ace_tools_open as tools
import pickle
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations
from collections import defaultdict
from konlpy.tag import Okt
#------------------------------------------------------------------------------
def rawdata_to_cleandata(data_file):
    with open(f'{data_file}', 'r', encoding='utf-8') as file:
        content = file.read()

    content = "".join(content.split())
    sentences = re.split(r'[.!?]', content)

    # ë¹ˆ ë¬¸ì¥ ì œê±° ë° í°ë”°ì˜´í‘œ ì œê±°
    clean_sentences = [re.sub(r'"', '', sentence) for sentence in sentences if sentence]

    spacing = Spacing()
    
    sentences = clean_sentences
    # ê³µë°± ë³µì›
    spaced_sentences = [spacing(sentence.strip()) for sentence in sentences]
    with open('clean_data.txt', 'w', encoding='utf-8') as output_file:
        output_file.write("\n".join(spaced_sentences))
#------------------------------------------------------------------------------
def book_to_pkl_nouns(data_file):
    file_path = f"{data_file}"
    with open(file_path, "r", encoding="utf-8") as f:
        lines = f.readlines()  # ì¤„ ë‹¨ìœ„ë¡œ ì½ê¸°

    stopwords = {"ê·¸","ê·¸ë…€","ê²ƒ","ë•Œ"}

    okt = Okt()
    tokenized_sentences = []
    filtered_words = []  # ë¶ˆìš©ì–´ ì œê±°ëœ ë‹¨ì–´ë§Œ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸

    for idx, line in enumerate(lines):
        nouns = okt.nouns(line)  # ëª…ì‚¬ë§Œ ì¶”ì¶œ

        # ğŸ”¹ ë¶ˆìš©ì–´ ì œê±° (ì»´í”„ë¦¬í—¨ì…˜ ì—†ì´ ì‘ì„±)
        filtered_nouns = []
        for word in nouns:
            if word not in stopwords:
                filtered_nouns.append(word)

        filtered_nouns = list(set(filtered_nouns))  # ì¤‘ë³µ ì œê±°-> í•œ ë¬¸ì¥ì— ì—¬ëŸ¬ê°œì˜ ê²ƒì´ ìˆë”ë¼ë„ ë¬´ì‹œ
        #ë‚˜ëŠ” ì´ê²Œ êµ³ì´ ìˆì–´ì•¼ í•˜ë‚˜ ì‹¶ê¸°ëŠ” í•¨

        tokenized_sentences.append({"index": idx, "num_nouns": len(filtered_nouns), "nouns": filtered_nouns})
        
        # ëª¨ë“  ë‹¨ì–´ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        filtered_words.extend(filtered_nouns)

    # ê²°ê³¼ ì €ì¥ (ë¶ˆìš©ì–´ ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥)
    with open("nouns_lines.pkl", "wb") as f:
        pickle.dump(tokenized_sentences, f)
#------------------------------------------------------------------------------
def nouns_ì‹ ë¢°ë„(pkl_file):
    with open(pkl_file, "rb") as f:
        sentences = pickle.load(f)

   # ëª…ì‚¬ ë“±ì¥ íšŸìˆ˜ ë° ì¡°í•© ë¹ˆë„ ê³„ì‚°
    word_pairs = defaultdict(int)
    word_counts = defaultdict(int)
    # ì´ê±°ëŠ” ì²˜ìŒ ë“±ì¥í•˜ëŠ” ìŒ(pairs)ë„ ì•ˆì „í•˜ê²Œ ì¹´ìš´íŠ¸ë¥¼ í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” ì½”ë“œ
    # -> ê¸°ì¡´ì˜ dictë¥¼ ì“°ë©´ ì—†ëŠ”ê±°ì— ëŒ€í•´ 0ìœ¼ë¡œ ì²˜ë¦¬ë¥¼ ì•ˆí•´ì£¼ê¸° ë•Œë¬¸ì— ë¬¸ì œê°€ ìƒê¸°ëŠ” ê²ƒì´ë‹¤. 
    total_sentences = len(sentences)

    for entry in sentences:
        nouns = entry["nouns"]  # ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°

        # ê°œë³„ ëª…ì‚¬ ë“±ì¥ íšŸìˆ˜ ê³„ì‚° -> ì´ê±°ëŠ” í•˜ë‚˜ ì¦‰ A,B ì´ëŸ°ê±°êµ¬ë‚˜
        for word in nouns:
            word_counts[word] += 1


        # ëª…ì‚¬ ìŒ ë“±ì¥ íšŸìˆ˜ ê³„ì‚° -> ê·¸ëŸ¬ë©´ ì´ê²Œ ì „ì²´êµ¬ë‚˜ ì¦‰ A,Bë‘˜ë‹¤ í•¨ê»˜ ë“±ì¥í•œ ìˆ˜
        for pair in combinations(nouns, 2):
            word_pairs[pair] += 1

    # ì‹ ë¢°ë„ ê³„ì‚°
    confidence_values = []
    for pair, support_AB in word_pairs.items():
        A, B = pair
        # ì´ê±°ëŠ” ì™œ í•œê±°ì•¼?

        support_A = word_counts.get(A, 0)   # Aì˜ ì§€ì§€ë„(ë“±ì¥í•œ íšŸìˆ˜)-> í‚¤ ê°’ìœ¼ë¡œ ê°’ì„ ê°€ì ¸ì˜¤ëŠ” ë°©ì‹
        support_B = word_counts.get(B, 0)   # Bì˜ ì§€ì§€ë„(ë“±ì¥í•œ íšŸìˆ˜)


        confidence_A_to_B = support_AB / support_A if support_A > 0 else 0# ì´ê±°ëŠ” í˜¹ì‹œ 0ìœ¼ë¡œ ë‚˜ëˆŒê¹Œë´ ë§Œë“  ì˜ˆë¹„ ì¥ì¹˜
        confidence_B_to_A = support_AB / support_B if support_B > 0 else 0# ì´ê±°ëŠ” í˜¹ì‹œ 0ìœ¼ë¡œ ë‚˜ëˆŒê¹Œë´ ë§Œë“  ì˜ˆë¹„ ì¥ì¹˜

        confidence_values.append((pair, confidence_A_to_B, confidence_B_to_A))

    confidence_df = pd.DataFrame(confidence_values, columns=["ë‹¨ì–´ìŒ", "ì‹ ë¢°ë„(Aâ†’B)", "ì‹ ë¢°ë„(Bâ†’A)"])

    # CSV ì €ì¥
    confidence_df.to_csv("ëª…ì‚¬_ì‹ ë¢°ë„.csv", index=False, encoding="utf-8-sig")
#------------------------------------------------------------------------------
def predict_top_five(user_input):
    data = pd.read_csv("ëª…ì‚¬_ì‹ ë¢°ë„.csv")
    data_A_B = []
    data_A_B_index = []
    data_B_A = []
    data_B_A_index = []
    # ì´ë ‡ê²Œ 2ê°œë¡œ ë‚˜ëˆˆ ì´ìœ ëŠ”
    # "('ì†Œë¦¬', 'ë„ˆ')",0.04672897196261682,0.02262443438914027
    # "('ë„ˆ', 'ì†Œë¦¬')",0.01809954751131222,0.037383177570093455
    # ì´ìƒí•˜ê²Œ ë‹¤ë¥´ë„¤ ì´ëŸ¬ë©´ ë¬´ì‹œë¥¼ ëª»í•˜ì§€

    for i in range(len(data["ë‹¨ì–´ìŒ"])):
        # ì´ìƒí•˜ê²Œ 1.0ì´ ë„˜ëŠ”ê²Œ ìˆë‹¤ ì…
        # if data["ì‹ ë¢°ë„(Aâ†’B)"][i] != 1 and data["ì‹ ë¢°ë„(Aâ†’B)"][i] >= 0.7 and data["ì‹ ë¢°ë„(Aâ†’B)"][i] != 2.0:
            data_A_B_index.append(data["ì‹ ë¢°ë„(Aâ†’B)"][i])
            data_A_B.append(data["ë‹¨ì–´ìŒ"][i].split("'")) 
        # if data["ì‹ ë¢°ë„(Bâ†’A)"][i] != 1 and data["ì‹ ë¢°ë„(Bâ†’A)"][i] >= 0.7 and data["ì‹ ë¢°ë„(Bâ†’A)"][i] != 2.0:
            data_B_A_index.append(data["ì‹ ë¢°ë„(Bâ†’A)"][i])
            data_B_A.append(data["ë‹¨ì–´ìŒ"][i].split("'"))
    # filtered_nouns = list(set(filtered_nouns)
    # ë‚´ê°€ ì´ê±°ë¥¼ ì˜ ì²˜ë¦¬ë¥¼ ì•ˆí•´ì„œ ìˆ«ìê°€ 1.0ì„ ë„˜ì–´ê°€ëŠ”ê²Œ ìˆëŠ”ê±´ê°€?
    # ì… ì´ê±° ë•Œë¬¸ì´êµ¬ë§Œ

    data_list = []
    data_index= []
    temp_list = []

    for i in range(len(data_A_B)):
        if data_A_B[i][1] == user_input:
            for i_2 in np.arange(0.99, 0, -0.01):  # 0.5 ì´ìƒë§Œ
                if data_A_B_index[i] >= i_2 and data_A_B_index[i] != 1.0:
                    temp_list.append((data_A_B[i][3], data_A_B_index[i]))
                    break

    for i in range(len(data_B_A)):
        if data_B_A[i][1] == user_input:
            for i_2 in np.arange(0.99, 0, -0.01):
                if data_B_A_index[i] >= i_2 and data_B_A_index[i] != 1.0:
                    temp_list.append((data_B_A[i][3], data_B_A_index[i]))
                    break


    # 1. ì‹ ë¢°ë„ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    sorted_temp = sorted(temp_list, key=lambda x: x[1], reverse=True)

    # 2. ìƒìœ„ 5ê°œë§Œ ì¶”ì¶œ
    top_5_temp = sorted_temp[:5]

    # 3. ë‹¤ì‹œ temp_listì— ì €ì¥
    temp_list = top_5_temp

    # ë¶„ë¦¬í•´ì„œ ì €ì¥
    for item in temp_list:
        data_list.append(item[0])
        data_index.append(item[1])

    for word, score in zip(data_list, data_index):
        print(word, score)

    